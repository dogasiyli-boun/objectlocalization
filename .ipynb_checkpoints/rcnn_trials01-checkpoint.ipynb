{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4549a7a2",
   "metadata": {},
   "source": [
    "#### https://blog.paperspace.com/object-localization-using-pytorch-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4df2735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import importlib as impL\n",
    "import tutorial01 as t01\n",
    "from helperFuncs import is_cuda_available, check_and_return_device\n",
    "import localization_example_helper_funcs as t01_hf\n",
    "is_cuda_available()\n",
    "device = check_and_return_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc82cc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Images Count: 960, Validation Images Count: 240\n"
     ]
    }
   ],
   "source": [
    "labels, boxes, img_list = t01.load_data()\n",
    "dataloader, valdataloader = t01.preprocess_data(img_list, boxes, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81044dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Model to be trained is created from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doga/anaconda3/envs/objectLocalization/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0:Accuracy 51.250/ loss:6.665/ time:0.049 mins\n",
      "Epoch   1:Accuracy 51.250/ loss:6.444/ time:0.039 mins\n",
      "Epoch   2:Accuracy 48.750/ loss:6.231/ time:0.038 mins\n",
      "Epoch   3:Accuracy 51.250/ loss:5.679/ time:0.039 mins\n",
      "Epoch   4:Accuracy 51.250/ loss:5.643/ time:0.039 mins\n",
      "Epoch   5:Accuracy 51.250/ loss:5.622/ time:0.039 mins\n",
      "Epoch   6:Accuracy 50.833/ loss:5.630/ time:0.039 mins\n",
      "Epoch   7:Accuracy 49.167/ loss:5.635/ time:0.039 mins\n",
      "Epoch   8:Accuracy 52.917/ loss:5.632/ time:0.039 mins\n",
      "Epoch   9:Accuracy 50.000/ loss:5.636/ time:0.040 mins\n",
      "Epoch  10:Accuracy 50.833/ loss:5.636/ time:0.039 mins\n",
      "Epoch  11:Accuracy 50.833/ loss:5.634/ time:0.039 mins\n",
      "Epoch  12:Accuracy 53.750/ loss:5.633/ time:0.038 mins\n",
      "Epoch  13:Accuracy 49.583/ loss:5.634/ time:0.039 mins\n",
      "Epoch  14:Accuracy 49.583/ loss:5.627/ time:0.039 mins\n",
      "Epoch  15:Accuracy 49.167/ loss:5.632/ time:0.040 mins\n",
      "Epoch  16:Accuracy 48.750/ loss:5.642/ time:0.038 mins\n",
      "Epoch  17:Accuracy 53.333/ loss:5.633/ time:0.039 mins\n",
      "Epoch  18:Accuracy 52.083/ loss:5.632/ time:0.039 mins\n",
      "Epoch  19:Accuracy 48.750/ loss:5.650/ time:0.039 mins\n",
      "Epoch  20:Accuracy 54.167/ loss:5.625/ time:0.039 mins\n",
      "Epoch  21:Accuracy 49.583/ loss:5.620/ time:0.040 mins\n",
      "Epoch  22:Accuracy 52.917/ loss:5.626/ time:0.040 mins\n",
      "Epoch  23:Accuracy 49.583/ loss:5.625/ time:0.039 mins\n",
      "Epoch  24:Accuracy 51.667/ loss:5.642/ time:0.040 mins\n",
      "Epoch  25:Accuracy 56.250/ loss:5.609/ time:0.039 mins\n",
      "Epoch  26:Accuracy 50.833/ loss:5.642/ time:0.039 mins\n",
      "Epoch  27:Accuracy 52.083/ loss:5.590/ time:0.039 mins\n",
      "Epoch  28:Accuracy 53.750/ loss:5.611/ time:0.040 mins\n",
      "Epoch  29:Accuracy 57.500/ loss:5.583/ time:0.040 mins\n",
      "Epoch  30:Accuracy 57.083/ loss:5.555/ time:0.039 mins\n",
      "Epoch  31:Accuracy 56.250/ loss:5.586/ time:0.039 mins\n",
      "Epoch  32:Accuracy 51.250/ loss:5.598/ time:0.039 mins\n",
      "Epoch  33:Accuracy 55.833/ loss:5.559/ time:0.039 mins\n",
      "Epoch  34:Accuracy 55.833/ loss:5.519/ time:0.039 mins\n",
      "Epoch  35:Accuracy 52.500/ loss:5.656/ time:0.039 mins\n",
      "Epoch  36:Accuracy 63.333/ loss:5.519/ time:0.038 mins\n",
      "Epoch  37:Accuracy 51.250/ loss:5.615/ time:0.039 mins\n",
      "Epoch  38:Accuracy 59.167/ loss:5.495/ time:0.039 mins\n",
      "Epoch  39:Accuracy 60.417/ loss:5.534/ time:0.039 mins\n",
      "Epoch  40:Accuracy 62.500/ loss:5.494/ time:0.038 mins\n",
      "Epoch  41:Accuracy 59.167/ loss:5.478/ time:0.038 mins\n",
      "Epoch  42:Accuracy 57.083/ loss:5.500/ time:0.038 mins\n",
      "Epoch  43:Accuracy 60.417/ loss:5.398/ time:0.038 mins\n",
      "Epoch  44:Accuracy 61.250/ loss:5.428/ time:0.039 mins\n",
      "Epoch  45:Accuracy 56.667/ loss:5.505/ time:0.039 mins\n",
      "Epoch  46:Accuracy 62.500/ loss:5.389/ time:0.039 mins\n",
      "Epoch  47:Accuracy 54.583/ loss:5.496/ time:0.039 mins\n",
      "Epoch  48:Accuracy 64.167/ loss:5.371/ time:0.039 mins\n",
      "Epoch  49:Accuracy 58.750/ loss:5.431/ time:0.038 mins\n"
     ]
    }
   ],
   "source": [
    "model1, acc_list1 = t01_hf.train(dataloader, valdataloader, model=None, num_of_epochs=50, start_from_scratch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d677cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(12, 24, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv4): Conv2d(24, 48, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv5): Conv2d(48, 192, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (class_fc1): Linear(in_features=1728, out_features=240, bias=True)\n",
       "  (class_fc2): Linear(in_features=240, out_features=120, bias=True)\n",
       "  (class_out): Linear(in_features=120, out_features=2, bias=True)\n",
       "  (box_fc1): Linear(in_features=1728, out_features=240, bias=True)\n",
       "  (box_fc2): Linear(in_features=240, out_features=120, bias=True)\n",
       "  (box_out): Linear(in_features=120, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb24b092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=240, out_features=120, bias=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.class_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1f8761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9949b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = model1.class_fc2\n",
    "backbone.out_channels = 120\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "roi_pooler = MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)\n",
    "mask_roi_pooler = MultiScaleRoIAlign(featmap_names=['0'], output_size=14, sampling_ratio=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cd1e096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskRCNN(\n",
       "  (transform): GeneralizedRCNNTransform()\n",
       "  (backbone): Linear(in_features=240, out_features=120, bias=True)\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(120, 9, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(120, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=5880, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "    (mask_roi_pool): MultiScaleRoIAlign()\n",
       "    (mask_head): MaskRCNNHeads(\n",
       "      (mask_fcn1): Conv2d(120, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu2): ReLU(inplace=True)\n",
       "      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu3): ReLU(inplace=True)\n",
       "      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu4): ReLU(inplace=True)\n",
       "    )\n",
       "    (mask_predictor): MaskRCNNPredictor(\n",
       "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask_fcn_logits): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.detection import MaskRCNN\n",
    "model_rc = MaskRCNN(backbone,\n",
    "                 num_classes=2, rpn_anchor_generator=anchor_generator,\n",
    "                 box_roi_pool=roi_pooler, mask_roi_pool=mask_roi_pooler)\n",
    "model_rc.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd250460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "x = [torch.rand(3, 300, 400).to(device), torch.rand(3, 500, 400).to(device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95dc1752",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_img_path1 = t01_hf.get_random_img()\n",
    "rand_img_path2 = t01_hf.get_random_img()\n",
    "img1 = cv2.imread(rand_img_path1)\n",
    "img2 = cv2.imread(rand_img_path2)\n",
    "img1 = img1.to(device)\n",
    "img2 = img2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99587cf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [6144 x 1088], m2: [240 x 120] at /opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/THC/generic/THCTensorMathBlas.cu:290",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9247/2618994099.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_rc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/objectLocalization/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/objectLocalization/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/objectLocalization/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/objectLocalization/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/objectLocalization/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [6144 x 1088], m2: [240 x 120] at /opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/THC/generic/THCTensorMathBlas.cu:290"
     ]
    }
   ],
   "source": [
    "predictions = model_rc([img1, img2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5d282c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "objectLocalization",
   "language": "python",
   "name": "objectlocalization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
